{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "plt.rcParams['figure.figsize'] = [20,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prot_div(left, right):\n",
    "    if right == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return left / right\n",
    "\n",
    "def if_func(condition, state1, state2):\n",
    "    if condition:\n",
    "        return state1\n",
    "    else:\n",
    "        return state2\n",
    "\n",
    "def read_experiment_results(full_path):\n",
    "    dir_name = full_path\n",
    "    results = []\n",
    "    it = 0\n",
    "    try:\n",
    "        while True:\n",
    "            with open(dir_name + 'iteration_' + str(it) + '.json') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                if it % 1 == 0:\n",
    "                    print(it)\n",
    "                results.append(data)\n",
    "            it += 1\n",
    "    except:\n",
    "        print(\"Finished reading \", full_path)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_number = range(1, 33)\n",
    "os_string = \"w10\"\n",
    "results = []\n",
    "if os_string == \"osx\":\n",
    "    path = \"/Users/soren/Google Drive/My Drive/results/autolr-2022-06-30/run_\"\n",
    "elif os_string == \"w10\":\n",
    "    path = \"C:\\\\Users\\\\lamec\\\\WORK\\\\journal\\\\dumps\\\\tensorflow\\\\run_\"\n",
    "    #path = \"C:\\\\Users\\\\lamec\\\\Desktop\\\\results\\\\mutation_level\\\\run_\"\n",
    "for i in run_number: \n",
    "    results.append(read_experiment_results(path + str(i) + '/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_phenotype import smart_phenotype, readable_phenotype\n",
    "import random\n",
    "def genealogy_string(results):\n",
    "    import graphviz\n",
    "    epochs = np.arange(len(results[0]))\n",
    "    lineage = []\n",
    "    fitness_color = []\n",
    "    occurences = {}\n",
    "    rendered = set()\n",
    "    prune_cutoff = 20\n",
    "    string = \"\"\"digraph genealogy{\n",
    "fontname=\"Helvetica,Arial,sans-serif\"\n",
    "node [fontname=\"Helvetica,Arial,sans-serif\"]\n",
    "edge [fontname=\"Helvetica,Arial,sans-serif\"]\n",
    "# page = \"8.2677165,11.692913\" ;\n",
    "ratio = \"auto\" ;\n",
    "mincross = 2.0 ;\n",
    "label = \"Genealogy\" ;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    connections_string = \"\"\n",
    "    for iteration in epochs:     \n",
    "        for result in results:\n",
    "            for indiv in result[iteration]:\n",
    "                if \"parent\" in indiv:\n",
    "                    for parent in indiv[\"parent\"]:\n",
    "                        if parent in occurences:\n",
    "                            occurences[parent] += 1\n",
    "                        else: \n",
    "                            occurences[parent] = 1\n",
    "                        if indiv['id'] in occurences:\n",
    "                            occurences[indiv['id']] += 1\n",
    "                        else: \n",
    "                            occurences[indiv['id']] = 1  \n",
    "                        lineage.append((parent, indiv['id'], indiv['fitness'] * -1))\n",
    "                fitness_color.append((indiv['id'], indiv['fitness'] * -1))\n",
    "    for parent, child, fitness in set(lineage):\n",
    "        if fitness > 0.12 and occurences[parent] > prune_cutoff and occurences[child] > prune_cutoff:\n",
    "            connections_string+=(f'\\n \"{parent}\" -> \"{child}\" ;')    \n",
    "            rendered.add(parent)\n",
    "            rendered.add(child)\n",
    "    for id, fitness in set(fitness_color):\n",
    "        if fitness > 0.12 and id in rendered:\n",
    "            string+=(f'\\n \"{id}\" [style=filled,fillcolor=\"#{format(int(255-fitness*255), \"x\")}{format(int(255-fitness*255), \"x\")}ff\", width={fitness*10}, height={fitness*10}] ;')\n",
    "\n",
    "    string += connections_string + \"\\n }\"\n",
    "    with open(\"graph.dot\", \"w\") as f:\n",
    "        print(string,file=f)\n",
    "    src = graphviz.Source(string)\n",
    "    src.render('doctest-output/graph.gv', view=True).replace('\\\\', '/')\n",
    "    return string\n",
    "\n",
    "#print(genealogy_string(results))\n",
    "def load_archive(path, run_number, generation):\n",
    "    with open(path + str(run_number) + '\\\\z-archive_' + str(generation) + \".json\", 'r') as f:\n",
    "        archive = json.load(f)\n",
    "    return archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (M, N).\n",
    "    row_labels\n",
    "        A list or array of length M with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length N with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "def annotate_heatmap(im, data=None, valfmt=\"{x:.2f}\",\n",
    "                     textcolors=(\"black\", \"white\"),\n",
    "                     threshold=None, **textkw):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A pair of colors.  The first is used for values below a threshold,\n",
    "        the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max())/2.\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\",\n",
    "              verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "def trim_phenotype(phenotype):\n",
    "    if \"shape\" in phenotype:\n",
    "        phenotype = phenotype.replace(\", shape=shape, dtype=tf.float32\", \"\")\n",
    "        phenotype = phenotype.replace(\"tf.math.\", \"\")\n",
    "        phenotype = phenotype.replace(\"tf.\", \"\")\n",
    "        functions = phenotype.split(r'lambda shape,  alpha')\n",
    "\n",
    "    elif \"size\" in phenotype:\n",
    "        phenotype = phenotype.replace(\", size=size, dtype=torch.float32\", \"\")\n",
    "        phenotype = phenotype.replace(\"torch.\", \"\")        \n",
    "        functions = phenotype.split(r'lambda size, alpha')\n",
    "    \n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "    return functions \n",
    "\n",
    "def math_phenotype(phenotype):\n",
    "    functions = trim_phenotype(phenotype)\n",
    "    alpha_func_string = functions[1][8:-2]\n",
    "    beta_func_string =functions[2][14:-2] \n",
    "    sigma_func_string =functions[3][21:-2] \n",
    "    grad_func_string = functions[-1][21:]\n",
    "    \n",
    "    return grad_func_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, symbol, parent, child_count):\n",
    "        self.symbol = symbol\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.child = None\n",
    "        self.parent = parent\n",
    "        self.child_count = child_count\n",
    "            \n",
    "    def insert(self, child):\n",
    "        if self.child_count == 2:\n",
    "            if self.left is None:\n",
    "                self.left = child\n",
    "            elif self.left.is_full() == False:\n",
    "                #print(f\"inserting in {self.left.symbol}\")\n",
    "                self.left.insert(child)\n",
    "            elif self.right is None:\n",
    "                self.right = child\n",
    "            elif self.right.is_full() == False:\n",
    "                #print(f\"inserting in {self.right.symbol}\")\n",
    "                self.right.insert(child)\n",
    "            else:\n",
    "                raise Exception(f\"All two children are full:{self.symbol}({self.left.symbol}, {self.right.symbol}) [{self.get_root().to_string()}]\")\n",
    "        elif self.child_count == 1:\n",
    "            if self.child is None:\n",
    "                self.child = child\n",
    "            elif self.child.is_full() == False:\n",
    "                #print(f\"inserting in {self.child.symbol}\")\n",
    "                self.child.insert(child)\n",
    "            else:\n",
    "                raise Exception(f\"Child is full: {self.symbol}({self.child.symbol}) [{self.get_root().to_string()}]\")\n",
    "        else:\n",
    "            raise Exception(f\"Insert called on terminal: {self.symbol} [{self.get_root().to_string()}]\")\n",
    "            \n",
    "    def is_full(self):\n",
    "        #print(f\"Calling is full on {self.symbol}\")\n",
    "        full = 0\n",
    "        if self.child_count == 2:\n",
    "            if self.left is None:\n",
    "                full = False\n",
    "            elif self.right is None:\n",
    "                full = False\n",
    "            elif self.left.is_full() and self.right.is_full():\n",
    "                full = True\n",
    "            else: \n",
    "                full = False\n",
    "        elif self.child_count == 1:\n",
    "            if self.child is None:\n",
    "                full = False\n",
    "            elif self.child.is_full():\n",
    "                full = True\n",
    "            else:\n",
    "                full = False\n",
    "        elif self.child_count == 0:\n",
    "            full = True\n",
    "        else:\n",
    "            raise Exception(f\"Symbol {self.symbol} does not 0, 1 or 2 children\")\n",
    "        return full\n",
    "        \n",
    "    def get_next(self):\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def to_string(self):\n",
    "        string = \"\"\n",
    "        if self.child_count == 2:\n",
    "            string +=\"(\" \n",
    "            if self.left is None:\n",
    "                string += \"_\"\n",
    "            else:\n",
    "                string += self.left.to_string()\n",
    "            string += self.symbol\n",
    "            if self.right is None:\n",
    "                string += \"_\"\n",
    "            else:\n",
    "                string += self.right.to_string()\n",
    "            string += \")\"\n",
    "            \n",
    "        elif self.child_count == 1:\n",
    "            string += self.symbol + \"(\" \n",
    "            if self.child is None:\n",
    "                string += \"_\"\n",
    "            else:\n",
    "                string += self.child.to_string()\n",
    "            string += \")\"\n",
    "        else:\n",
    "            string += self.symbol\n",
    "        return string\n",
    "\n",
    "    def get_root(self):\n",
    "        if self.parent is not None:\n",
    "            return self.parent.get_root()\n",
    "        else:\n",
    "            return self\n",
    "        \n",
    "        \n",
    "def process_results(results, negative_fit=True, run_number=None):\n",
    "    epochs = np.arange(len(results[0]))\n",
    "    #print(epochs)\n",
    "    best_individuals = {}\n",
    "    averages_all = []\n",
    "    bests_all = []\n",
    "    stds_all = []\n",
    "    best_of_all = []\n",
    "    stds_best_all = [] \n",
    "    indivs = []\n",
    "    indiv_fits = []\n",
    "    indivs_epochs = []\n",
    "    indivs_per_epoch = []\n",
    "    boa_fit = 0\n",
    "    boa_indiv = None\n",
    "    run_best = np.zeros(len(results))\n",
    "    for iteration in epochs:\n",
    "        averages_all.append([])\n",
    "        bests_all.append([])\n",
    "        stds_all.append([])\n",
    "        indivs_per_epoch.append([])\n",
    "        best_of_all.append(0)\n",
    "        stds_best_all.append(0)    \n",
    "        run = 0  \n",
    "        for result in results:\n",
    "            all_fits = []\n",
    "            best = 0\n",
    "            gen_best = 0\n",
    "            if iteration < len(result):\n",
    "                for indiv in result[iteration]:\n",
    "                    if negative_fit:\n",
    "                        indiv[\"fitness\"] *= -1\n",
    "                    indiv['run'] = run\n",
    "                    if 'smart_phenotype' not in indiv:\n",
    "                        indiv['smart_phenotype'] = smart_phenotype(indiv[\"phenotype\"])\n",
    "                    indivs.append(indiv)\n",
    "                    indiv_fits.append(indiv['fitness'])\n",
    "                    indivs_epochs.append(iteration)\n",
    "                    indivs_per_epoch[iteration].append(indiv['fitness'])\n",
    "                    if indiv['fitness'] > run_best[run]:\n",
    "                        run_best[run] = indiv['fitness']\n",
    "                        if run_number is None:\n",
    "                            print(f'------NEW BEST IN RUN------\\n{iteration}-{indiv[\"id\"]}-{boa_fit}\\n{smart_phenotype(indiv[\"phenotype\"])}\\n--------------------')\n",
    "                        else:\n",
    "                            print(f'------NEW BEST IN RUN {run_number[run]}------\\nEPOCH:{iteration}\\nID:{indiv[\"id\"]}-{boa_fit}\\n{readable_phenotype(indiv[\"phenotype\"])}\\n{indiv[\"phenotype\"]}--------------------')      \n",
    "                    if indiv['fitness'] > best:\n",
    "                        best = indiv['fitness']\n",
    "                        if indiv['fitness'] > boa_fit:\n",
    "                            boa_fit = best\n",
    "                            best_individuals[iteration] = {\"fitness\": boa_fit, \"phenotype\": indiv[\"phenotype\"], \"smart_phenotype\": smart_phenotype(indiv[\"phenotype\"])}\n",
    "                    best = best if indiv['fitness'] < best else indiv['fitness']\n",
    "                    all_fits.append(indiv['fitness'])\n",
    "                averages_all[iteration].append(np.average(all_fits))\n",
    "                stds_all[iteration].append(np.std(all_fits))\n",
    "                bests_all[iteration].append(best)\n",
    "            run += 1\n",
    "        stds_best_all[iteration] = np.std(bests_all[iteration]) \n",
    "        stds_all[iteration] = np.std(averages_all[iteration])\n",
    "        averages_all[iteration] = np.average(averages_all[iteration])\n",
    "        best_of_all[iteration] = np.max(bests_all[iteration])  \n",
    "        bests_all[iteration] = np.average(bests_all[iteration])\n",
    "    return {\"averages_all\": averages_all, \"bests_all\": bests_all, \"indivs\": indivs, \"indivs_epochs\": indivs_epochs, \"epochs\": epochs}\n",
    "\n",
    "def plot_fit(epochs, averages_all, bests_all): \n",
    "    plt.figure(facecolor='#eff2f1')\n",
    "    \n",
    "    ax = plt.axes()\n",
    "    ax.set_facecolor(color=\"#eff2f1\")\n",
    "    ax.spines['bottom'].set_color('#08415c')\n",
    "    ax.spines['top'].set_color('#08415c')\n",
    "    ax.spines['left'].set_color('#08415c')\n",
    "    ax.spines['right'].set_color('#08415c')\n",
    "    ax.xaxis.label.set_color('#08415c')\n",
    "    ax.yaxis.label.set_color('#08415c')\n",
    "    ax.tick_params(axis='x', colors='#08415c')\n",
    "    ax.tick_params(axis='y', colors=\"#08415c\")\n",
    "    plt.plot(epochs, averages_all, label='population average', color=\"#7796cb\")\n",
    "    #plt.fill_between(epochs, [i + j for i, j in zip(averages_all, stds_all)], [i - j for i, j in zip(averages_all, stds_all)], alpha=0.2)\n",
    "    plt.plot(epochs, bests_all, label='best average', color=\"#EFA00B\")   \n",
    "    #plt.fill_between(epochs, [i + j for i, j in zip(bests_all, stds_best_all)], [i - j for i, j in zip(bests_all, stds_best_all)], alpha=0.2)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.savefig(f'best_average_evolution{run_number}.png')\n",
    "    plt.savefig(f'best_average_evolution{run_number}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "def pop_density_heatmap(epochs, indivs, rows=10, columns=10, fit_floor=0, fit_ceil=1, gen_floor=0.0, gen_ceil=100.0):\n",
    "    epoch_len = gen_ceil - gen_floor\n",
    "    heatmap_matrix = np.zeros((rows, columns), dtype=np.int32)\n",
    "    fit_range = fit_ceil - fit_floor\n",
    "    row_labels = [f\"{(fit_range) / rows * x + fit_floor:.2f},{(fit_range) / rows * (x + 1) + fit_floor:.2f}\" for x in range(rows)]\n",
    "    columns_labels = [f\"{int((epoch_len) / columns * x)},{int((epoch_len) / columns * (x + 1))}\" for x in range(columns)]\n",
    "    for epoch, indiv in zip(epochs, indivs):\n",
    "        if indiv[\"fitness\"] >= fit_floor and indiv[\"fitness\"] <= fit_ceil and epoch > gen_floor and epoch < gen_ceil:\n",
    "            row_index = int((indiv[\"fitness\"] - fit_floor) / fit_range * rows)\n",
    "            column_index = int(float(epoch) / float(epoch_len) * columns)\n",
    "            heatmap_matrix[row_index][column_index] += 1\n",
    "    fig, ax = plt.subplots()\n",
    "    im, cbar = heatmap(heatmap_matrix, row_labels, col_labels=columns_labels, ax=ax, cbarlabel=\"indiv count\", cmap=\"Greys\")\n",
    "    texts = annotate_heatmap(im,valfmt=\"{x:.1E}\")\n",
    "    fig.tight_layout()\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Fitness')\n",
    "    plt.title('Individuals Density Heatmap (Generations vs Fitness)')\n",
    "    plt.show()\n",
    "\n",
    "def unique_indivs_heatmap(epochs, indivs, rows=10, columns=10, fit_floor=0, fit_ceil=1, gen_floor=0.0, gen_ceil=100.0):\n",
    "    epoch_len = gen_ceil - gen_floor\n",
    "    heatmap_matrix = np.zeros((rows, columns), dtype=np.int32)\n",
    "    unique_indivs_record = {}\n",
    "    fit_range = fit_ceil - fit_floor\n",
    "    row_labels = [f\"{(fit_range) / rows * x + fit_floor:.2f},{(fit_range) / rows * (x + 1) + fit_floor:.2f}\" for x in range(rows)]\n",
    "    columns_labels = [f\"{int((epoch_len) / columns * x)},{int((epoch_len) / columns * (x + 1))}\" for x in range(columns)]\n",
    "    for epoch, indiv in zip(epochs, indivs):\n",
    "        if indiv[\"fitness\"] >= fit_floor and indiv[\"fitness\"] <= fit_ceil and epoch > gen_floor and epoch < gen_ceil:\n",
    "            row_index = int((indiv[\"fitness\"] - fit_floor) / fit_range * rows)\n",
    "            column_index = int(float(epoch) / float(epoch_len) * columns)\n",
    "            if row_index not in unique_indivs_record:\n",
    "                unique_indivs_record[row_index] = {}\n",
    "            if column_index not in unique_indivs_record[row_index]:\n",
    "                unique_indivs_record[row_index][column_index] = []\n",
    "            if indiv[\"smart_phenotype\"] not in unique_indivs_record[row_index][column_index]:\n",
    "                if epoch == epochs[-1]:\n",
    "                    print(indiv[\"smart_phenotype\"])\n",
    "                unique_indivs_record[row_index][column_index].append(indiv[\"smart_phenotype\"])\n",
    "                heatmap_matrix[row_index][column_index] += 1\n",
    "    fig, ax = plt.subplots()\n",
    "    im, cbar = heatmap(heatmap_matrix, row_labels, col_labels=columns_labels, ax=ax, cbarlabel=\"unique indiv count\", cmap=\"Greys\")\n",
    "    texts = annotate_heatmap(im)\n",
    "    fig.tight_layout()\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Fitness')\n",
    "    plt.title('Unique Individuals Density Heatmap (Generations vs Fitness)')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_results(results, run_number=run_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns, rows = (40, 10)\n",
    "plt.rcParams['figure.figsize'] = [columns, rows]\n",
    "plot_fit(data[\"epochs\"], data['averages_all'], data['bests_all'])\n",
    "\n",
    "fit_floor = 0.0\n",
    "fit_ceil = 1.0\n",
    "gen_floor = 0.0\n",
    "gen_ceil = float(len(data['epochs']))\n",
    "pop_density_heatmap(data[\"indivs_epochs\"], data[\"indivs\"], rows=rows, columns=columns, fit_floor=fit_floor, fit_ceil=fit_ceil, gen_floor=gen_floor, gen_ceil=gen_ceil)\n",
    "unique_indivs_heatmap(data[\"indivs_epochs\"], data[\"indivs\"], rows=rows, columns=columns, fit_floor=fit_floor, fit_ceil=fit_ceil, gen_floor=gen_floor, gen_ceil=gen_ceil)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_list = []\n",
    "for i in run_number: \n",
    "    it = 1 \n",
    "    try:\n",
    "        while True:\n",
    "            archive = load_archive(path, i, it)\n",
    "            it += 1\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print(f\"loading archive {it - 1} for run {i}\")\n",
    "        archive = load_archive(path, i, it - 1)\n",
    "        for x in archive:\n",
    "            if 'fitness' in archive[x]:\n",
    "                archive_list.append([x, archive[x], archive[x]['fitness'], i])\n",
    "    except:\n",
    "        print(f\"Run {i} has no archive\")\n",
    "archive_list.sort(key=lambda x : x[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_list\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(archive_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "while it < 10:\n",
    "    print(archive_list[it])\n",
    "    it+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def turn_to_expr(phenotype, tree):\n",
    "    if phenotype == \"\":\n",
    "        return tree\n",
    "    \n",
    "    if phenotype[0:9] == \"multiply(\":\n",
    "        #print(\"multiply\")\n",
    "        phenotype = phenotype[9:]\n",
    "        node = Node(\"*\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:14] == \"divide_no_nan(\":\n",
    "        phenotype = phenotype[14:]\n",
    "        node = Node(\"/\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:4] == \"add(\":\n",
    "        phenotype = phenotype[4:]\n",
    "        node = Node(\"+\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:4] == \"pow(\":\n",
    "        phenotype = phenotype[4:]\n",
    "        node = Node(\"^\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:9] == \"subtract(\":\n",
    "        phenotype = phenotype[9:]\n",
    "        node = Node(\"-\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:4] == \"grad\":\n",
    "        phenotype = phenotype[4:]\n",
    "        node = Node(\"grad\", tree, 0)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:4] == \"beta\":\n",
    "        phenotype = phenotype[4:]\n",
    "        node = Node(\"beta\", tree, 0)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:5] == \"alpha\":\n",
    "        phenotype = phenotype[5:]\n",
    "        node = Node(\"alpha\", tree, 0)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:5] == \"sigma\":\n",
    "        phenotype = phenotype[5:]\n",
    "        node = Node(\"sigma\", tree, 0)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:9] == \"negative(\":\n",
    "        phenotype = phenotype[9:]\n",
    "        node = Node(\"-\", tree, 1)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:5] == \"sqrt(\":\n",
    "        phenotype = phenotype[5:]\n",
    "        node = Node(\"sqrt\", tree, 1)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:7] == \"square(\":\n",
    "        phenotype = phenotype[7:]\n",
    "        node = Node(\"square\", tree, 1)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:2] == \", \":\n",
    "        phenotype = phenotype[2:]\n",
    "        return turn_to_expr(phenotype, tree)     \n",
    "    elif phenotype[0] == \")\":\n",
    "        phenotype = phenotype[1:]\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:9] == \"constant(\":\n",
    "        if phenotype[9:12] == \"0.0\" or phenotype[9:13] == \"1.0)\" or phenotype[9:13] == \"1.0,\":\n",
    "            node = Node(phenotype[9:12], tree, 0)\n",
    "            phenotype = phenotype[12:]\n",
    "            tree.insert(node)\n",
    "            tree = tree.get_next()           \n",
    "        else:\n",
    "            node = Node(phenotype[9:9+14], tree, 0)\n",
    "            phenotype = phenotype[9+14:]\n",
    "            tree.insert(node)\n",
    "            tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    else:\n",
    "        raise Exception(phenotype)\n",
    "        \n",
    "def math_phenotype(phenotype):\n",
    "    functions = trim_phenotype(phenotype)\n",
    "    alpha_func_string = functions[1][8:-2]\n",
    "    beta_func_string =functions[2][14:-2] \n",
    "    sigma_func_string =functions[3][21:-2] \n",
    "    grad_func_string = functions[-1][21:]\n",
    "    \n",
    "    for x in [alpha_func_string, beta_func_string, sigma_func_string, grad_func_string]:\n",
    "        print(x)    \n",
    "        turn_to_expr(x, Node(\"\", None, 1)).to_string()\n",
    "\n",
    "    return grad_func_string\n",
    "\n",
    "for indiv in data['indivs']:\n",
    "    math_phenotype(indiv['phenotype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"multiply(\"\n",
    "print(a[0:9])\n",
    "print(a[9:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import csv\n",
    "from pickle import NONE\n",
    "from utils.data_functions import load_fashion_mnist_training, load_cifar10_training, load_mnist_training, select_fashion_mnist_training\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "from keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from optimizers.custom_optimizer import CustomOptimizer\n",
    "import datetime\n",
    "experiment_time = datetime.datetime.now()\n",
    "\n",
    "cached_dataset = None\n",
    "cached_model = None\n",
    "\n",
    "def train_model_tensorflow_cifar10(phen_params):\n",
    "    phen, params = phen_params\n",
    "    validation_size = params['VALIDATION_SIZE']\n",
    "    fitness_size =params['FITNESS_SIZE'] \n",
    "    batch_size = params['BATCH_SIZE']\n",
    "    epochs = params['EPOCHS']\n",
    "    patience = params['PATIENCE']\n",
    "\n",
    "    # Note that globals are borderline -- consider an object or a closure \n",
    "    # deliberately using globals() to make it ugly...\n",
    "    if globals()['cached_dataset'] == None:\n",
    "        globals()['cached_dataset'] = load_cifar10_training(validation_size=validation_size, test_size=fitness_size)\n",
    "    \n",
    "    if globals()['cached_model'] == None:\n",
    "        globals()['cached_model'] = load_model(params['MODEL'], compile=False)\n",
    "        \n",
    "    # we assume validation and test sets are deterministic\n",
    "    dataset = globals()['cached_dataset'] \n",
    "    model = tf.keras.models.clone_model(globals()['cached_model'])\n",
    "\n",
    "    weights = model.get_weights()\n",
    "    model.set_weights(weights)  \n",
    "\n",
    "    # optimizer is constant aslong as phen doesn't changed?\n",
    "    # -> opportunity to cache opt and compiled model\n",
    "    opt = CustomOptimizer(phen=phen, model=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=patience, restore_best_weights=True)\n",
    "\n",
    "    score = model.fit(dataset['x_train'], dataset['y_train'],\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=2,\n",
    "        validation_data=(dataset['x_val'], dataset['y_val']),\n",
    "        validation_steps= validation_size // batch_size,\n",
    "        callbacks=[\n",
    "            early_stop\n",
    "        ])\n",
    "\n",
    "    K.clear_session()\n",
    "    results = {}\n",
    "    for metric in score.history:\n",
    "        results[metric] = []\n",
    "        for n in score.history[metric]:\n",
    "            results[metric].append(n)\n",
    "    test_score = model.evaluate(x=dataset['x_test'],y=dataset[\"y_test\"], verbose=0, callbacks=[keras.callbacks.History()])\n",
    "    return test_score[-1], results\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0357653c69581ed709ad33b04b35df511a3a4051acb4aa15a7fef2257addd2ab"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
