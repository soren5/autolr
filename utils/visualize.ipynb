{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "matplotlib.rcParams[\"pdf.fonttype\"] = 42\n",
    "matplotlib.rcParams[\"ps.fonttype\"] = 42\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 20]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions to read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_experiment_results(full_path):\n",
    "    dir_name = full_path\n",
    "    results = []\n",
    "    it = 0\n",
    "    try:\n",
    "        while True:\n",
    "            with open(dir_name + \"iteration_\" + str(it) + \".json\") as json_file:\n",
    "                data = json.load(json_file)\n",
    "                if it % 1 == 0:\n",
    "                    print(it)\n",
    "                results.append(data)\n",
    "            it += 1\n",
    "    except:\n",
    "        print(\"Finished reading \", full_path)\n",
    "    return results\n",
    "\n",
    "\n",
    "# extract number from file\n",
    "def extract_number(f):\n",
    "    s = re.findall(\"\\d+$\", f)\n",
    "    return (int(s[0]) if s else -1, f)\n",
    "\n",
    "\n",
    "\"\"\"Human sorting of files \"\"\"\n",
    "\"\"\"From https://nedbatchelder.com/blog/200712/human_sorting.html \"\"\"\n",
    "\n",
    "\n",
    "def tryint(s):\n",
    "    \"\"\"\n",
    "    Return an int if possible, or `s` unchanged.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "\n",
    "def alphanum_key(s):\n",
    "    \"\"\"\n",
    "    Turn a string into a list of string and number chunks.\n",
    "\n",
    "    >>> alphanum_key(\"z23a\")\n",
    "    [\"z\", 23, \"a\"]\n",
    "\n",
    "    \"\"\"\n",
    "    return [tryint(c) for c in re.split(\"([0-9]+)\", s)]\n",
    "\n",
    "\n",
    "def human_sort(l):\n",
    "    \"\"\"\n",
    "    Sort a list in the way that humans expect.\n",
    "    \"\"\"\n",
    "    l.sort(key=alphanum_key)\n",
    "\n",
    "\n",
    "def read_experiment_results_agnostic(full_path):\n",
    "    results = []\n",
    "    #filenames = glob.glob(pathname=\"iteration_?*\", root_dir=full_path)\n",
    "    filenames = glob.glob(pathname=os.path.join(full_path, \"iteration_?*\"))\n",
    "    filenames_ordered = human_sort(filenames)\n",
    "    try:\n",
    "        for file in filenames:\n",
    "            json_file = open(Path(os.path.join(full_path, file)))\n",
    "            data = json.load(json_file)\n",
    "            if extract_number(file)[0] % 1 == 0:\n",
    "                print(file)\n",
    "            results.append(data)\n",
    "    except:\n",
    "        print(\"Error in reading results \", full_path)\n",
    "    return results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify where to get results from and load (system agnostic version).\n",
    "\n",
    "It is necessary to:\n",
    "\n",
    "1. set the folder in `result_folde_path_from_root` as a list of all the folders starting from the root down to the one containing the runs (assumes root is the parent dir of the dir containing this script)\n",
    "2. set the runs numbers to analyze as a range `run_number` between 1 and N where n is the number of runs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming folder with data for different tasks is in root directory\n",
    "def load_results(folder, task, run_number):\n",
    "    result_folder_path_from_root = os.path.join(folder, task)\n",
    "    results_agnostic = []\n",
    "    path = os.path.join(\n",
    "        os.path.dirname(os.path.abspath(os.curdir)), result_folder_path_from_root\n",
    "    )\n",
    "    print(path)\n",
    "\n",
    "    for i in run_number:\n",
    "        file_path = os.path.join(path, \"run_\" + str(i))\n",
    "        results_agnostic.append(read_experiment_results_agnostic(file_path))\n",
    "    results_agnostic.append({\"task\": task})\n",
    "    return results_agnostic\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_number = range(1, 33)\n",
    "task = \"tensorflow\"\n",
    "folder = \"C:\\\\Users\\\\lamec\\\\WORK\\\\journal\\\\dumps\"\n",
    "\n",
    "results_agnostic = load_results(folder=folder, task=task, run_number=run_number)\n",
    "data_agnostic=results_agnostic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defintion of extra functions to plot data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genealogy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_phenotype import smart_phenotype, readable_phenotype\n",
    "import random\n",
    "\n",
    "\n",
    "def genealogy_string(results):\n",
    "    import graphviz\n",
    "\n",
    "    epochs = np.arange(len(results[0]))\n",
    "    lineage = []\n",
    "    fitness_color = []\n",
    "    occurences = {}\n",
    "    rendered = set()\n",
    "    prune_cutoff = 20\n",
    "    string = \"\"\"digraph genealogy{\n",
    "fontname=\"Helvetica,Arial,sans-serif\"\n",
    "node [fontname=\"Helvetica,Arial,sans-serif\"]\n",
    "edge [fontname=\"Helvetica,Arial,sans-serif\"]\n",
    "# page = \"8.2677165,11.692913\" ;\n",
    "ratio = \"auto\" ;\n",
    "mincross = 2.0 ;\n",
    "label = \"Genealogy\" ;\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    connections_string = \"\"\n",
    "    for iteration in epochs:\n",
    "        for result in results:\n",
    "            for indiv in result[iteration]:\n",
    "                if \"parent\" in indiv:\n",
    "                    for parent in indiv[\"parent\"]:\n",
    "                        if parent in occurences:\n",
    "                            occurences[parent] += 1\n",
    "                        else:\n",
    "                            occurences[parent] = 1\n",
    "                        if indiv[\"id\"] in occurences:\n",
    "                            occurences[indiv[\"id\"]] += 1\n",
    "                        else:\n",
    "                            occurences[indiv[\"id\"]] = 1\n",
    "                        lineage.append((parent, indiv[\"id\"], indiv[\"fitness\"] * -1))\n",
    "                fitness_color.append((indiv[\"id\"], indiv[\"fitness\"] * -1))\n",
    "    for parent, child, fitness in set(lineage):\n",
    "        if (\n",
    "            fitness > 0.12\n",
    "            and occurences[parent] > prune_cutoff\n",
    "            and occurences[child] > prune_cutoff\n",
    "        ):\n",
    "            connections_string += f'\\n \"{parent}\" -> \"{child}\" ;'\n",
    "            rendered.add(parent)\n",
    "            rendered.add(child)\n",
    "    for id, fitness in set(fitness_color):\n",
    "        if fitness > 0.12 and id in rendered:\n",
    "            string += f'\\n \"{id}\" [style=filled,fillcolor=\"#{format(int(255-fitness*255), \"x\")}{format(int(255-fitness*255), \"x\")}ff\", width={fitness*10}, height={fitness*10}] ;'\n",
    "\n",
    "    string += connections_string + \"\\n }\"\n",
    "    with open(\"graph.dot\", \"w\") as f:\n",
    "        print(string, file=f)\n",
    "    src = graphviz.Source(string)\n",
    "    src.render(\"doctest-output/graph.gv\", view=True).replace(\"\\\\\", \"/\")\n",
    "    return string\n",
    "\n",
    "\n",
    "# print(genealogy_string(results))\n",
    "def load_archive(path, run_number, generation):\n",
    "    with open(\n",
    "        path + str(run_number) + \"\\\\z-archive_\" + str(generation) + \".json\", \"r\"\n",
    "    ) as f:\n",
    "        archive = json.load(f)\n",
    "    return archive\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines class node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, symbol, parent, child_count):\n",
    "        self.symbol = symbol\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.child = None\n",
    "        self.parent = parent\n",
    "        self.child_count = child_count\n",
    "\n",
    "    def insert(self, child):\n",
    "        if self.child_count == 2:\n",
    "            if self.left is None:\n",
    "                self.left = child\n",
    "            elif self.left.is_full() == False:\n",
    "                # print(f\"inserting in {self.left.symbol}\")\n",
    "                self.left.insert(child)\n",
    "            elif self.right is None:\n",
    "                self.right = child\n",
    "            elif self.right.is_full() == False:\n",
    "                # print(f\"inserting in {self.right.symbol}\")\n",
    "                self.right.insert(child)\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f\"All two children are full:{self.symbol}({self.left.symbol}, {self.right.symbol}) [{self.get_root().to_string()}]\"\n",
    "                )\n",
    "        elif self.child_count == 1:\n",
    "            if self.child is None:\n",
    "                self.child = child\n",
    "            elif self.child.is_full() == False:\n",
    "                # print(f\"inserting in {self.child.symbol}\")\n",
    "                self.child.insert(child)\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f\"Child is full: {self.symbol}({self.child.symbol}) [{self.get_root().to_string()}]\"\n",
    "                )\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Insert called on terminal: {self.symbol} [{self.get_root().to_string()}]\"\n",
    "            )\n",
    "\n",
    "    def is_full(self):\n",
    "        # print(f\"Calling is full on {self.symbol}\")\n",
    "        full = 0\n",
    "        if self.child_count == 2:\n",
    "            if self.left is None:\n",
    "                full = False\n",
    "            elif self.right is None:\n",
    "                full = False\n",
    "            elif self.left.is_full() and self.right.is_full():\n",
    "                full = True\n",
    "            else:\n",
    "                full = False\n",
    "        elif self.child_count == 1:\n",
    "            if self.child is None:\n",
    "                full = False\n",
    "            elif self.child.is_full():\n",
    "                full = True\n",
    "            else:\n",
    "                full = False\n",
    "        elif self.child_count == 0:\n",
    "            full = True\n",
    "        else:\n",
    "            raise Exception(f\"Symbol {self.symbol} does not 0, 1 or 2 children\")\n",
    "        return full\n",
    "\n",
    "    def get_next(self):\n",
    "        return self\n",
    "\n",
    "    def to_string(self):\n",
    "        string = \"\"\n",
    "        if self.child_count == 2:\n",
    "            string += \"(\"\n",
    "            if self.left is None:\n",
    "                string += \"_\"\n",
    "            else:\n",
    "                string += self.left.to_string()\n",
    "            string += self.symbol\n",
    "            if self.right is None:\n",
    "                string += \"_\"\n",
    "            else:\n",
    "                string += self.right.to_string()\n",
    "            string += \")\"\n",
    "\n",
    "        elif self.child_count == 1:\n",
    "            string += self.symbol + \"(\"\n",
    "            if self.child is None:\n",
    "                string += \"_\"\n",
    "            else:\n",
    "                string += self.child.to_string()\n",
    "            string += \")\"\n",
    "        else:\n",
    "            string += self.symbol\n",
    "        return string\n",
    "\n",
    "    def get_root(self):\n",
    "        if self.parent is not None:\n",
    "            return self.parent.get_root()\n",
    "        else:\n",
    "            return self\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate all the data, it might take a few minutes to run\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(results, negative_fit=True, run_number=None):\n",
    "    # print(epochs)\n",
    "    best_individuals = {}\n",
    "    averages_all = []\n",
    "    averages = []\n",
    "    bests_all = []\n",
    "    bests = []\n",
    "    stds_all = []\n",
    "    best_of_all = []\n",
    "    stds_best_all = []\n",
    "    indivs = []\n",
    "    indiv_fits = []\n",
    "    indivs_epochs = []\n",
    "    indivs_per_epoch = []\n",
    "    unique_indivs = []\n",
    "    boa_fit = 0\n",
    "    run_best = np.zeros(len(results))\n",
    "\n",
    "    # make sure last elemt corresponds to the task on which the individuals evolved\n",
    "    # store the task to name the processed_results later\n",
    "    # and take it out from data\n",
    "    task = results[-1]\n",
    "    assert type(task) == dict\n",
    "    assert len(task) == 1\n",
    "    assert \"task\" in task\n",
    "    results = results[:-1]\n",
    "    assert type(results[-1]) != dict\n",
    "\n",
    "    # see the max n of gens in results\n",
    "    generations = np.arange(len(max(results, key=len)))\n",
    "\n",
    "    # loop over generations\n",
    "    processed_result = loop_over_generations(\n",
    "        results,\n",
    "        negative_fit,\n",
    "        run_number,\n",
    "        best_individuals,\n",
    "        averages_all,\n",
    "        averages,\n",
    "        bests_all,\n",
    "        bests,\n",
    "        stds_all,\n",
    "        best_of_all,\n",
    "        stds_best_all,\n",
    "        indivs,\n",
    "        indiv_fits,\n",
    "        indivs_epochs,\n",
    "        indivs_per_epoch,\n",
    "        boa_fit,\n",
    "        run_best,\n",
    "        generations,\n",
    "        unique_indivs\n",
    "    )\n",
    "\n",
    "    # name the data after the task\n",
    "    processed_result.update(task)\n",
    "\n",
    "    return processed_result\n",
    "\n",
    "\n",
    "def loop_over_generations(\n",
    "    results,\n",
    "    negative_fit,\n",
    "    run_number,\n",
    "    best_individuals,\n",
    "    averages_all,\n",
    "    averages,\n",
    "    bests_all,\n",
    "    bests,\n",
    "    stds_all,\n",
    "    best_of_all,\n",
    "    stds_best_all,\n",
    "    indivs,\n",
    "    indiv_fits,\n",
    "    indivs_epochs,\n",
    "    indivs_per_epoch,\n",
    "    boa_fit,\n",
    "    run_best,\n",
    "    generations,\n",
    "    unique_indivs\n",
    "):\n",
    "    for generation in generations:\n",
    "        averages.append([])\n",
    "        bests.append([])\n",
    "        stds_all.append([])\n",
    "        indivs_per_epoch.append([])\n",
    "        best_of_all.append(0)\n",
    "        stds_best_all.append(0)\n",
    "        run = 0\n",
    "\n",
    "        # loop over runs\n",
    "        for result in results:\n",
    "            all_fits = []\n",
    "            best = 0\n",
    "            if len(unique_indivs) == run:\n",
    "                unique_indivs.append([])\n",
    "            if generation < len(result):\n",
    "            # loop over inds\n",
    "                unique_inds_run = []\n",
    "                for indiv in result[generation]:\n",
    "                    indiv[\"run\"] = run\n",
    "                    if negative_fit:\n",
    "                        indiv[\"fitness\"] *= -1\n",
    "                    if \"smart_phenotype\" not in indiv:\n",
    "                        indiv[\"smart_phenotype\"] = smart_phenotype(indiv[\"phenotype\"])\n",
    "                    if indiv[\"smart_phenotype\"] not in unique_inds_run:\n",
    "                        unique_inds_run.append(indiv)\n",
    "                    indivs.append(indiv)\n",
    "                    indiv_fits.append(indiv[\"fitness\"])\n",
    "                    indivs_epochs.append(generation)\n",
    "                    indivs_per_epoch[generation].append(indiv[\"fitness\"])\n",
    "                    if indiv[\"fitness\"] > run_best[run]:\n",
    "                        run_best[run] = indiv[\"fitness\"]\n",
    "                    if indiv[\"fitness\"] > best:\n",
    "                        best = indiv[\"fitness\"]\n",
    "                        if indiv[\"fitness\"] > boa_fit:\n",
    "                            boa_fit = best\n",
    "                            best_individuals[generation] = {\n",
    "                                \"fitness\": boa_fit,\n",
    "                                \"phenotype\": indiv[\"phenotype\"],\n",
    "                                \"smart_phenotype\": indiv[\"smart_phenotype\"],\n",
    "                            }\n",
    "                    best = best if indiv[\"fitness\"] < best else indiv[\"fitness\"]\n",
    "                    all_fits.append(indiv[\"fitness\"])\n",
    "                averages[generation].append(np.average(all_fits))\n",
    "\n",
    "                unique_indivs[run].append([])    \n",
    "                unique_indivs[run][generation] = unique_inds_run\n",
    "\n",
    "                stds_all[generation].append(np.std(all_fits))\n",
    "                bests[generation].append(best)\n",
    "            run += 1\n",
    "\n",
    "        stds_best_all[generation] = np.std(bests[generation])\n",
    "        stds_all[generation] = np.std(averages[generation])\n",
    "        averages_all.append(np.average(averages[generation]))\n",
    "        best_of_all[generation] = np.max(bests[generation])\n",
    "        bests_all.append(np.average(bests[generation]))\n",
    "\n",
    "        processed_result = {\n",
    "            \"averages\": averages,\n",
    "            \"bests\": bests,\n",
    "            \"averages_all\": averages_all,\n",
    "            \"bests_all\": bests_all,\n",
    "            \"indivs\": indivs,\n",
    "            \"indivs_epochs\": indivs_epochs,\n",
    "            \"epochs\": generations,\n",
    "            \"unique_indivs\": unique_indivs\n",
    "        }\n",
    "\n",
    "    return processed_result\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def loop_over_runs(\n",
    "    results,\n",
    "    negative_fit,\n",
    "    run_number,\n",
    "    best_individuals,\n",
    "    averages_all,\n",
    "    averages,\n",
    "    bests_all,\n",
    "    bests,\n",
    "    stds_all,\n",
    "    best_of_all,\n",
    "    stds_best_all,\n",
    "    indivs,\n",
    "    indiv_fits,\n",
    "    indivs_epochs,\n",
    "    indivs_per_epoch,\n",
    "    boa_fit,\n",
    "    run_best,\n",
    "    unique_indivs\n",
    "):\n",
    "\n",
    "\n",
    "\n",
    "    # loop over runs\n",
    "    for result in results:\n",
    "        run = 0\n",
    "        unique_indivs.append([])\n",
    "\n",
    "        for generation in len(result):\n",
    "            \n",
    "            averages.append([])\n",
    "            bests.append([])\n",
    "            stds_all.append([])\n",
    "            indivs_per_epoch.append([])\n",
    "            best_of_all.append(0)\n",
    "            stds_best_all.append(0)\n",
    "            unique_indivs[run].append([])\n",
    "            all_fits = []\n",
    "            best = 0\n",
    "            if generation < len(result):\n",
    "            # loop over inds\n",
    "                for indiv in result[generation]:\n",
    "                    if negative_fit:\n",
    "                        indiv[\"fitness\"] *= -1\n",
    "                    indiv[\"run\"] = run\n",
    "                    if \"smart_phenotype\" not in indiv:\n",
    "                        indiv[\"smart_phenotype\"] = smart_phenotype(indiv[\"phenotype\"])\n",
    "                    if indiv[\"smart_phenotype\"] not in unique_indivs[run][generation]:\n",
    "                        unique_indivs[run][generation].append(indiv)\n",
    "                    indivs.append(indiv)\n",
    "                    indiv_fits.append(indiv[\"fitness\"])\n",
    "                    indivs_epochs.append(generation)\n",
    "                    indivs_per_epoch[generation].append(indiv[\"fitness\"])\n",
    "                    if indiv[\"fitness\"] > run_best[run]:\n",
    "                        run_best[run] = indiv[\"fitness\"]\n",
    "                    if indiv[\"fitness\"] > best:\n",
    "                        best = indiv[\"fitness\"]\n",
    "                        if indiv[\"fitness\"] > boa_fit:\n",
    "                            boa_fit = best\n",
    "                            best_individuals[generation] = {\n",
    "                                \"fitness\": boa_fit,\n",
    "                                \"phenotype\": indiv[\"phenotype\"],\n",
    "                                \"smart_phenotype\": indiv[\"smart_phenotype\"],\n",
    "                            }\n",
    "                    best = best if indiv[\"fitness\"] < best else indiv[\"fitness\"]\n",
    "                    all_fits.append(indiv[\"fitness\"])\n",
    "                averages[generation].append(np.average(all_fits))\n",
    "                stds_all[generation].append(np.std(all_fits))\n",
    "                bests[generation].append(best)\n",
    "                stds_best_all[generation] = np.std(bests[generation])\n",
    "                stds_all[generation] = np.std(averages[generation])\n",
    "                averages_all.append(np.average(averages[generation]))\n",
    "                best_of_all[generation] = np.max(bests[generation])\n",
    "                bests_all.append(np.average(bests[generation]))\n",
    "\n",
    "    processed_result = {\n",
    "        \"averages\": averages,\n",
    "        \"bests\": bests,\n",
    "        \"averages_all\": averages_all,\n",
    "        \"bests_all\": bests_all,\n",
    "        \"indivs\": indivs,\n",
    "        \"indivs_epochs\": indivs_epochs,\n",
    "        \"unique_indivs\": unique_indivs\n",
    "    }\n",
    "\n",
    "    return processed_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create plots using the cell below\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available plots:\n",
    "\n",
    "[LINE PLOT] Show fitness over generations:\n",
    "\n",
    "`plot_fit(data[\"epochs\"], data['averages_all'], data['bests_all'])`\n",
    "\n",
    "[HEAT MAP] Show the distribution of population fitness over generations:\n",
    "\n",
    "`pop_density_heatmap(data[\"indivs_epochs\"], data[\"indivs\"], rows=rows, columns=columns, fit_floor=fit_floor, fit_ceil=fit_ceil, gen_floor=gen_floor, gen_ceil=gen_ceil)`\n",
    "\n",
    "[LINE PLOT] Unique individuals over generation:\n",
    "\n",
    "`unique_indivs_curve(data[\"indivs_epochs\"], data[\"indivs\"], fit_floor=fit_floor, fit_ceil=fit_ceil, gen_floor=gen_floor, gen_ceil=gen_ceil)`\n",
    "\n",
    "[HEAT MAP] Unique individuals distribution of fitness over generations:\n",
    "\n",
    "`unique_indivs_heatmap(data[\"indivs_epochs\"], data[\"indivs\"], rows=rows, columns=columns, fit_floor=fit_floor, fit_ceil=fit_ceil, gen_floor=gen_floor, gen_ceil=gen_ceil)`\n",
    "\n",
    "## You can filter the results in the last three plots using these settings:\n",
    "\n",
    "`fit_floor` - Only show results for individuals above this fitness\n",
    "\n",
    "`fit_ceil` - Only show results for individuals below this fitness\n",
    "\n",
    "`gen_floor` - Only show results after this generation (THIS MUST BE A FLOAT)\n",
    "\n",
    "`gen_ceiling` - Only show results before this generation (THIS MUST BE A FLOAT)\n",
    "\n",
    "You can also use `columns, rows` to adjust the ratio and detail of the heatmaps.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel=\"\", **kwargs):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a numpy array and two lists of labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data\n",
    "        A 2D numpy array of shape (M, N).\n",
    "    row_labels\n",
    "        A list or array of length M with the labels for the rows.\n",
    "    col_labels\n",
    "        A list or array of length N with the labels for the columns.\n",
    "    ax\n",
    "        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n",
    "        not provided, use current axes or create a new one.  Optional.\n",
    "    cbar_kw\n",
    "        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n",
    "    cbarlabel\n",
    "        The label for the colorbar.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to `imshow`.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar\n",
    "\n",
    "\n",
    "def annotate_heatmap(\n",
    "    im,\n",
    "    data=None,\n",
    "    valfmt=\"{x:.2f}\",\n",
    "    textcolors=(\"black\", \"white\"),\n",
    "    threshold=None,\n",
    "    **textkw\n",
    "):\n",
    "    \"\"\"\n",
    "    A function to annotate a heatmap.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im\n",
    "        The AxesImage to be labeled.\n",
    "    data\n",
    "        Data used to annotate.  If None, the image's data is used.  Optional.\n",
    "    valfmt\n",
    "        The format of the annotations inside the heatmap.  This should either\n",
    "        use the string format method, e.g. \"$ {x:.2f}\", or be a\n",
    "        `matplotlib.ticker.Formatter`.  Optional.\n",
    "    textcolors\n",
    "        A pair of colors.  The first is used for values below a threshold,\n",
    "        the second for those above.  Optional.\n",
    "    threshold\n",
    "        Value in data units according to which the colors from textcolors are\n",
    "        applied.  If None (the default) uses the middle of the colormap as\n",
    "        separation.  Optional.\n",
    "    **kwargs\n",
    "        All other arguments are forwarded to each call to `text` used to create\n",
    "        the text labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(data, (list, np.ndarray)):\n",
    "        data = im.get_array()\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    if threshold is not None:\n",
    "        threshold = im.norm(threshold)\n",
    "    else:\n",
    "        threshold = im.norm(data.max()) / 2.0\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = dict(horizontalalignment=\"center\", verticalalignment=\"center\")\n",
    "    kw.update(textkw)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n",
    "            texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "def trim_phenotype(phenotype):\n",
    "    if \"shape\" in phenotype:\n",
    "        phenotype = phenotype.replace(\", shape=shape, dtype=tf.float32\", \"\")\n",
    "        phenotype = phenotype.replace(\"tf.math.\", \"\")\n",
    "        phenotype = phenotype.replace(\"tf.\", \"\")\n",
    "        functions = phenotype.split(r\"lambda shape,  alpha\")\n",
    "\n",
    "    elif \"size\" in phenotype:\n",
    "        phenotype = phenotype.replace(\", size=size, dtype=torch.float32\", \"\")\n",
    "        phenotype = phenotype.replace(\"torch.\", \"\")\n",
    "        functions = phenotype.split(r\"lambda size, alpha\")\n",
    "\n",
    "    else:\n",
    "        raise Exception()\n",
    "\n",
    "    return functions\n",
    "\n",
    "\n",
    "def math_phenotype(phenotype):\n",
    "    functions = trim_phenotype(phenotype)\n",
    "    alpha_func_string = functions[1][8:-2]\n",
    "    beta_func_string = functions[2][14:-2]\n",
    "    sigma_func_string = functions[3][21:-2]\n",
    "    grad_func_string = functions[-1][21:]\n",
    "\n",
    "    return grad_func_string\n",
    "\n",
    "def plot_fit(epochs, averages_all, averages, bests_all, bests, task=None, folder=None):\n",
    "    \n",
    "    #fill unexistent values with 0s \n",
    "    max_len = len(max(averages, key = len))\n",
    "    for average_index in range(0, len(averages)):\n",
    "        if len(averages[average_index]) < max_len:\n",
    "            for index in range(len(averages[average_index]), max_len):\n",
    "                averages[average_index].append(0)\n",
    "                bests[average_index].append(0)\n",
    "\n",
    "    plt.figure(facecolor=\"#eff2f1\")\n",
    "\n",
    "    ax = plt.axes()\n",
    "    ax.set_facecolor(color=\"#eff2f1\")\n",
    "    ax.spines[\"bottom\"].set_color(\"#08415c\")\n",
    "    ax.spines[\"top\"].set_color(\"#08415c\")\n",
    "    ax.spines[\"left\"].set_color(\"#08415c\")\n",
    "    ax.spines[\"right\"].set_color(\"#08415c\")\n",
    "    ax.xaxis.label.set_color(\"#08415c\")\n",
    "    ax.yaxis.label.set_color(\"#08415c\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"#08415c\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"#08415c\")\n",
    "    plt.plot(epochs, averages_all, label=\"average\", color=\"#7796cb\")\n",
    "    plt.plot(\n",
    "        epochs, averages, label=\"populations averages\", color=\"#5596cb\", linewidth=0.5\n",
    "    )\n",
    "    # plt.fill_between(epochs, [i + j for i, j in zip(averages_all, stds_aall)], [i - j for i, j in zip(averages_all, stds_all)], alpha=0.2)\n",
    "    plt.plot(epochs, bests_all, label=\"best average\", color=\"#EFA00B\")\n",
    "    plt.plot(epochs, bests, label=\"bests\", color=\"#EFA20B\", linewidth=0.5)\n",
    "    # plt.fill_between(epochs, [i + j for i, j in zip(bests_all, stds_best_all)], [i - j for i, j in zip(bests_all, stds_best_all)], alpha=0.2)\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.savefig(folder / f\"best_average_evolution{run_number}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pop_density_heatmap(\n",
    "    epochs,\n",
    "    indivs,\n",
    "    rows=10,\n",
    "    columns=10,\n",
    "    fit_floor=0,\n",
    "    fit_ceil=1,\n",
    "    gen_floor=0.0,\n",
    "    gen_ceil=100.0,\n",
    "    task=None,\n",
    "    folder=None,\n",
    "):\n",
    "    epoch_len = gen_ceil - gen_floor\n",
    "    heatmap_matrix = np.zeros((rows, columns), dtype=np.int32)\n",
    "    fit_range = fit_ceil - fit_floor\n",
    "    row_labels = [\n",
    "        f\"{(fit_range) / rows * x + fit_floor:.2f},{(fit_range) / rows * (x + 1) + fit_floor:.2f}\"\n",
    "        for x in range(rows)\n",
    "    ]\n",
    "    columns_labels = [\n",
    "        f\"{int((epoch_len) / columns * x)},{int((epoch_len) / columns * (x + 1))}\"\n",
    "        for x in range(columns)\n",
    "    ]\n",
    "    for epoch, indiv in zip(epochs, indivs):\n",
    "        if (\n",
    "            indiv[\"fitness\"] >= fit_floor\n",
    "            and indiv[\"fitness\"] <= fit_ceil\n",
    "            and epoch > gen_floor\n",
    "            and epoch < gen_ceil\n",
    "        ):\n",
    "            row_index = int((indiv[\"fitness\"] - fit_floor) / fit_range * rows)\n",
    "            column_index = int(float(epoch) / float(epoch_len) * columns)\n",
    "            heatmap_matrix[row_index][column_index] += 1\n",
    "    fig, ax = plt.subplots()\n",
    "    im, cbar = heatmap(\n",
    "        heatmap_matrix,\n",
    "        row_labels,\n",
    "        col_labels=columns_labels,\n",
    "        ax=ax,\n",
    "        cbarlabel=\"indiv count\",\n",
    "        cmap=\"Greys\",\n",
    "    )\n",
    "    texts = annotate_heatmap(im, valfmt=\"{x:.1E}\")\n",
    "    fig.tight_layout()\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Fitness\")\n",
    "    plt.title(\"Individuals Density Heatmap (Generations vs Fitness)\")\n",
    "    plt.savefig(os.path.join(folder, f\"pop_density_evolution{run_number}.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def unique_indivs_heatmap(\n",
    "    epochs,\n",
    "    indivs,\n",
    "    rows=10,\n",
    "    columns=10,\n",
    "    fit_floor=0,\n",
    "    fit_ceil=1,\n",
    "    gen_floor=0.0,\n",
    "    gen_ceil=100.0,\n",
    "    task=None,\n",
    "    folder=None,\n",
    "):\n",
    "    epoch_len = gen_ceil - gen_floor\n",
    "    heatmap_matrix = np.zeros((rows, columns), dtype=np.int32)\n",
    "    unique_indivs_record = {}\n",
    "    fit_range = fit_ceil - fit_floor\n",
    "    row_labels = [\n",
    "        f\"{(fit_range) / rows * x + fit_floor:.2f},{(fit_range) / rows * (x + 1) + fit_floor:.2f}\"\n",
    "        for x in range(rows)\n",
    "    ]\n",
    "    columns_labels = [\n",
    "        f\"{int((epoch_len) / columns * x)},{int((epoch_len) / columns * (x + 1))}\"\n",
    "        for x in range(columns)\n",
    "    ]\n",
    "    for epoch, indiv in zip(epochs, indivs):\n",
    "        if (\n",
    "            indiv[\"fitness\"] >= fit_floor\n",
    "            and indiv[\"fitness\"] <= fit_ceil\n",
    "            and epoch > gen_floor\n",
    "            and epoch < gen_ceil\n",
    "        ):\n",
    "            row_index = int((indiv[\"fitness\"] - fit_floor) / fit_range * rows)\n",
    "            column_index = int(float(epoch) / float(epoch_len) * columns)\n",
    "            if row_index not in unique_indivs_record:\n",
    "                unique_indivs_record[row_index] = {}\n",
    "            if column_index not in unique_indivs_record[row_index]:\n",
    "                unique_indivs_record[row_index][column_index] = []\n",
    "            if (\n",
    "                indiv[\"smart_phenotype\"]\n",
    "                not in unique_indivs_record[row_index][column_index]\n",
    "            ):\n",
    "                if epoch == epochs[-1]:\n",
    "                    # print(indiv[\"smart_phenotype\"])\n",
    "                    pass\n",
    "                unique_indivs_record[row_index][column_index].append(\n",
    "                    indiv[\"smart_phenotype\"]\n",
    "                )\n",
    "                heatmap_matrix[row_index][column_index] += 1\n",
    "    fig, ax = plt.subplots()\n",
    "    im, cbar = heatmap(\n",
    "        heatmap_matrix,\n",
    "        row_labels,\n",
    "        col_labels=columns_labels,\n",
    "        ax=ax,\n",
    "        cbarlabel=\"unique indiv count\",\n",
    "        cmap=\"Greys\",\n",
    "    )\n",
    "    texts = annotate_heatmap(im)\n",
    "    fig.tight_layout()\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Fitness\")\n",
    "    plt.title(\"Unique Individuals Density Heatmap (Generations vs Fitness)\")\n",
    "    plt.savefig(os.path.join(folder, f\"pop_unique_ind_fit_evolution{run_number}.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def unique_indivs_curve(\n",
    "    epochs,\n",
    "    indivs,\n",
    "    fit_floor=0,\n",
    "    fit_ceil=1,\n",
    "    gen_floor=0.0,\n",
    "    gen_ceil=100.0,\n",
    "    task=None,\n",
    "    folder=None,\n",
    "):\n",
    "    rows = 1\n",
    "    epoch_len = gen_ceil - gen_floor\n",
    "    columns = int(epoch_len)\n",
    "    heatmap_matrix = np.zeros((rows, columns), dtype=np.int32)\n",
    "    unique_indivs_record = {}\n",
    "    fit_range = fit_ceil - fit_floor\n",
    "    row_labels = [\n",
    "        f\"{(fit_range) / rows * x + fit_floor:.2f},{(fit_range) / rows * (x + 1) + fit_floor:.2f}\"\n",
    "        for x in range(rows)\n",
    "    ]\n",
    "    columns_labels = [\n",
    "        f\"{int((epoch_len) / columns * x)},{int((epoch_len) / columns * (x + 1))}\"\n",
    "        for x in range(columns)\n",
    "    ]\n",
    "    for epoch, indiv in zip(epochs, indivs):\n",
    "        if (\n",
    "            indiv[\"fitness\"] >= fit_floor\n",
    "            and indiv[\"fitness\"] <= fit_ceil\n",
    "            and epoch > gen_floor\n",
    "            and epoch < gen_ceil\n",
    "        ):\n",
    "            row_index = int((indiv[\"fitness\"] - fit_floor) / fit_range * rows)\n",
    "            column_index = int(float(epoch) / float(epoch_len) * columns)\n",
    "            if row_index not in unique_indivs_record:\n",
    "                unique_indivs_record[row_index] = {}\n",
    "            if column_index not in unique_indivs_record[row_index]:\n",
    "                unique_indivs_record[row_index][column_index] = []\n",
    "            if (\n",
    "                indiv[\"smart_phenotype\"]\n",
    "                not in unique_indivs_record[row_index][column_index]\n",
    "            ):\n",
    "                if epoch == epochs[-1]:\n",
    "                    # print(indiv[\"smart_phenotype\"])\n",
    "                    pass\n",
    "                unique_indivs_record[row_index][column_index].append(\n",
    "                    indiv[\"smart_phenotype\"]\n",
    "                )\n",
    "                heatmap_matrix[row_index][column_index] += 1\n",
    "    # print([heatmap_matrix[row] for row in range(len(heatmap_matrix))])\n",
    "    plt.plot(\n",
    "        [x for x in range(columns)],\n",
    "        np.transpose([heatmap_matrix[row] for row in range(len(heatmap_matrix))]),\n",
    "    )\n",
    "    plt.xlabel(\"Generation\")\n",
    "    plt.ylabel(\"Number of Uniques\")\n",
    "    plt.title(\"Number of Unique Behaviours throughout Evolution\")\n",
    "    plt.savefig(os.path.join(folder, f\"unique_inds_evo{run_number}.png\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns, rows = (40, 10)\n",
    "plt.rcParams[\"figure.figsize\"] = [columns, rows]\n",
    "# plot_fit(data[\"epochs\"], data['averages_all'], data['bests_all'])\n",
    "\n",
    "fit_floor = 0.0\n",
    "fit_ceil = 1.0\n",
    "gen_floor = 0.0\n",
    "# gen_ceil = float(len(data['epochs']))\n",
    "gen_ceil = float(len(data_agnostic[\"epochs\"]))\n",
    "\n",
    "# unique_indivs_heatmap(data[\"indivs_epochs\"], data[\"indivs\"], rows=rows, columns=columns, fit_floor=fit_floor, fit_ceil=fit_ceil, gen_floor=gen_floor, gen_ceil=gen_ceil)\n",
    "plot_fit(\n",
    "    data_agnostic[\"epochs\"],\n",
    "    data_agnostic[\"averages_all\"],\n",
    "    data_agnostic[\"bests_all\"],\n",
    "    data_agnostic[\"task\"],\n",
    ")\n",
    "pop_density_heatmap(\n",
    "    data_agnostic[\"indivs_epochs\"],\n",
    "    data_agnostic[\"indivs\"],\n",
    "    rows=rows,\n",
    "    columns=columns,\n",
    "    fit_floor=fit_floor,\n",
    "    fit_ceil=fit_ceil,\n",
    "    gen_floor=gen_floor,\n",
    "    gen_ceil=gen_ceil,\n",
    "    task=data_agnostic[\"task\"],\n",
    ")\n",
    "unique_indivs_heatmap(\n",
    "    data_agnostic[\"indivs_epochs\"],\n",
    "    data_agnostic[\"indivs\"],\n",
    "    rows=rows,\n",
    "    columns=columns,\n",
    "    fit_floor=fit_floor,\n",
    "    fit_ceil=fit_ceil,\n",
    "    gen_floor=gen_floor,\n",
    "    gen_ceil=gen_ceil,\n",
    "    task=data_agnostic[\"task\"],\n",
    ")\n",
    "unique_indivs_curve(\n",
    "    data_agnostic[\"indivs_epochs\"],\n",
    "    data_agnostic[\"indivs\"],\n",
    "    fit_floor=fit_floor,\n",
    "    fit_ceil=fit_ceil,\n",
    "    gen_floor=gen_floor,\n",
    "    gen_ceil=gen_ceil,\n",
    "    task=data_agnostic[\"task\"],\n",
    ")\n",
    "# plt.vlines([0,2,6,9,11,16,23,29,30,31,35,36,42,45,47,54,55,75],0,100,colors='red')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the data and plot in one go\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loop(run_number, tasks, folders, over_folder):\n",
    "    for folder in folders:\n",
    "        for task in tasks:\n",
    "            folder_path = os.path.join(over_folder, folder)\n",
    "            subfold_path = Path(folder_path, task)\n",
    "            subfold_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            results = load_results(folder=folder_path, task=task, run_number=run_number)\n",
    "            data = process_results(results=results, run_number=run_number)\n",
    "\n",
    "            columns, rows = (40, 10)\n",
    "            plt.rcParams[\"figure.figsize\"] = [columns, rows]\n",
    "\n",
    "            fit_floor = 0.15\n",
    "            fit_ceil = 1.0\n",
    "            gen_floor = 0.0\n",
    "            gen_ceil = float(len(data[\"epochs\"]))\n",
    "\n",
    "            plot_fit(\n",
    "                data[\"epochs\"],\n",
    "                data[\"averages_all\"],\n",
    "                data[\"averages\"],\n",
    "                data[\"bests_all\"],\n",
    "                data[\"bests\"],\n",
    "                data[\"task\"],\n",
    "                folder=subfold_path,\n",
    "            )\n",
    "            pop_density_heatmap(\n",
    "                data[\"indivs_epochs\"],\n",
    "                data[\"indivs\"],\n",
    "                rows=rows,\n",
    "                columns=columns,\n",
    "                fit_floor=fit_floor,\n",
    "                fit_ceil=fit_ceil,\n",
    "                gen_floor=gen_floor,\n",
    "                gen_ceil=gen_ceil,\n",
    "                task=data[\"task\"],\n",
    "                folder=subfold_path,\n",
    "            )\n",
    "            unique_indivs_heatmap(\n",
    "                data[\"indivs_epochs\"],\n",
    "                data[\"indivs\"],\n",
    "                rows=rows,\n",
    "                columns=columns,\n",
    "                fit_floor=fit_floor,\n",
    "                fit_ceil=fit_ceil,\n",
    "                gen_floor=gen_floor,\n",
    "                gen_ceil=gen_ceil,\n",
    "                task=data[\"task\"],\n",
    "                folder=subfold_path,\n",
    "            )\n",
    "    # unique_indivs_curve(data[\"indivs_epochs\"], data[\"indivs\"], fit_floor=fit_floor, fit_ceil=fit_ceil, gen_floor=gen_floor, gen_ceil=gen_ceil, task = data['task'], folder = folder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = range(1, 31) #upper limit is number of runs + 1, assumes they exist\n",
    "folders = [\"many_runs_no_crossover\"] #this has to correspond to the name of the folders\n",
    "tasks = [\"fmni\"] #this has to correspond to the name of the folders\n",
    "over_folder = \"many_results\\\\\"\n",
    "\n",
    "plot_loop(run_number, tasks, folders, over_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run t-tests \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = range(1, 31)\n",
    "task1 = \"fmni\"\n",
    "folder1 = \"many_results\\\\many_runs\"\n",
    "folder2 = \"many_results\\\\many_runs_old_mut\"\n",
    "folder3 = \"many_results\\\\many_runs_no_crossover\"\n",
    "folder4 = \"many_results\\\\many_runs_new_crossover\"\n",
    "\n",
    "data_f1_t1 = process_results(\n",
    "    load_results(folder=folder1, task=task1, run_number=run_number)\n",
    ")\n",
    "data_f2_t1 = process_results(load_results(folder=folder2, task=task1 ,run_number=run_number))\n",
    "data_f3_t1 = process_results(load_results(folder=folder3, task=task1 ,run_number=run_number))\n",
    "data_f4_t1 = process_results(load_results(folder=folder4, task=task1 ,run_number=run_number))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on performance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run t-test using [this][link to test]\n",
    "\n",
    "[link to test]: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_final = 200\n",
    "\n",
    "print(\"NEW\")\n",
    "print(\"Bests over runs\")\n",
    "print(data_f1_t1[\"bests\"][gen_final])\n",
    "print(\"Best average over runs\")\n",
    "print(data_f1_t1[\"bests_all\"][gen_final])\n",
    "print(\"Best of all\")\n",
    "print(max(data_f1_t1[\"bests\"][gen_final]))\n",
    "print(\n",
    "    \"run\"\n",
    "    + \"% s\"\n",
    "    % (data_f1_t1[\"bests\"][gen_final].index(max(data_f1_t1[\"bests\"][gen_final])) + 1)\n",
    ")\n",
    "print(\"NEW_+_CROSSOVER\")\n",
    "print(\"Bests over runs\")\n",
    "print(data_f4_t1[\"bests\"][gen_final])\n",
    "print(\"Best average over runs\")\n",
    "print(data_f4_t1[\"bests_all\"][gen_final])\n",
    "print(\"Best of all\")\n",
    "print(max(data_f4_t1[\"bests\"][gen_final]))\n",
    "print(\n",
    "    \"run\"\n",
    "    + \"% s\"\n",
    "    % (data_f4_t1[\"bests\"][gen_final].index(max(data_f4_t1[\"bests\"][gen_final])) + 1)\n",
    ")\n",
    "print(\"OLD\")\n",
    "print(\"Bests over runs\")\n",
    "print(data_f2_t1[\"bests\"][gen_final])\n",
    "print(\"Best average over runs\")\n",
    "print(data_f2_t1[\"bests_all\"][gen_final])\n",
    "print(\"Best of all\")\n",
    "print(max(data_f2_t1[\"bests\"][gen_final]))\n",
    "print(\n",
    "    \"run\"\n",
    "    + \"% s\"\n",
    "    % (data_f2_t1[\"bests\"][gen_final].index(max(data_f2_t1[\"bests\"][gen_final])) + 1)\n",
    ")\n",
    "print(\"OLD_NO_CROSSOVER\")\n",
    "print(\"Bests over runs\")\n",
    "print(data_f3_t1[\"bests\"][gen_final])\n",
    "print(\"Best average over runs\")\n",
    "print(data_f3_t1[\"bests_all\"][gen_final])\n",
    "print(\"Best of all\")\n",
    "print(max(data_f3_t1[\"bests\"][gen_final]))\n",
    "print(\n",
    "    \"run\"\n",
    "    + \"% s\"\n",
    "    % (data_f3_t1[\"bests\"][gen_final].index(max(data_f3_t1[\"bests\"][gen_final])) + 1)\n",
    ")\n",
    "\n",
    "# bests\n",
    "bests_fmni_new_vs_old = stats.ttest_ind(\n",
    "    data_f1_t1[\"bests\"][gen_final], data_f2_t1[\"bests\"][gen_final], equal_var=False\n",
    ")\n",
    "bests_fmni_new_vs_old_no_crossover = stats.ttest_ind(\n",
    "    data_f1_t1[\"bests\"][gen_final], data_f3_t1[\"bests\"][gen_final], equal_var=False\n",
    ")\n",
    "bests_fmni_new_cr_vs_old = stats.ttest_ind(\n",
    "    data_f4_t1[\"bests\"][gen_final], data_f2_t1[\"bests\"][gen_final], equal_var=False\n",
    ")\n",
    "bests_fmni_new_cr_vs_old_no_crossover = stats.ttest_ind(\n",
    "    data_f4_t1[\"bests\"][gen_final], data_f3_t1[\"bests\"][gen_final], equal_var=False\n",
    ")\n",
    "\n",
    "# averages\n",
    "avg_fmni_new_vs_old = stats.ttest_ind(\n",
    "    data_f1_t1[\"averages\"][gen_final],\n",
    "    data_f2_t1[\"averages\"][gen_final],\n",
    "    equal_var=False,\n",
    ")\n",
    "avg_fmni_new_vs_old_no_crossover = stats.ttest_ind(\n",
    "    data_f1_t1[\"averages\"][gen_final],\n",
    "    data_f3_t1[\"averages\"][gen_final],\n",
    "    equal_var=False,\n",
    ")\n",
    "avg_fmni_new_cr_vs_old = stats.ttest_ind(\n",
    "    data_f4_t1[\"averages\"][gen_final],\n",
    "    data_f2_t1[\"averages\"][gen_final],\n",
    "    equal_var=False,\n",
    ")\n",
    "avg_fmni_new_cr_vs_old_no_crossover = stats.ttest_ind(\n",
    "    data_f4_t1[\"averages\"][gen_final],\n",
    "    data_f3_t1[\"averages\"][gen_final],\n",
    "    equal_var=False,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-tests results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests_fmni_new_vs_old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests_fmni_new_vs_old_no_crossover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests_fmni_new_cr_vs_old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bests_fmni_new_cr_vs_old_no_crossover\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fmni_new_vs_old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fmni_new_vs_old_no_crossover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fmni_new_cr_vs_old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_fmni_new_cr_vs_old_no_crossover\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on diversity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_inds_for_gen(data, gen_range, fit_floor):\n",
    "    indivs = data[\"indivs\"]\n",
    "    epochs = data[\"indivs_epochs\"]\n",
    "    ind_epochs = zip(epochs, indivs)\n",
    "    unique_counts = [0] * len(gen_range)\n",
    "    unique_phens = [[0]] * len(gen_range)\n",
    "\n",
    "    for epoch, indiv in ind_epochs:\n",
    "        if epoch in gen_range:\n",
    "            # offset index deepnding on first generation to be recorde so that it starts with 1\n",
    "            unique_index = epoch - gen_range.start\n",
    "            if indiv[\"smart_phenotype\"] not in unique_phens[unique_index]:\n",
    "                unique_phens[unique_index].append(indiv[\"smart_phenotype\"])\n",
    "                if indiv[\"fitness\"] >= fit_floor:\n",
    "                    unique_counts[unique_index] += 1\n",
    "\n",
    "    return unique_counts\n",
    "\n",
    "\n",
    "def find_last_unique_gen(data):\n",
    "    unique_indivs_1 = data[\"unique_indivs\"]\n",
    "    min_lengths = []\n",
    "    max_length = len(max(unique_indivs_1, key=len))\n",
    "\n",
    "    return min_lengths\n",
    "\n",
    "def find_last_unique_gen_in_common(data1, data2):\n",
    "    unique_indivs_1 = data1[\"unique_indivs\"]\n",
    "    unique_indivs_2 = data2[\"unique_indivs\"]\n",
    "    max_length = len(max(unique_indivs_1, key=len))\n",
    "    min_lengths = []\n",
    "    \n",
    "    # find minimum row in which number of elemnts is not the maximimu,\n",
    "    for run in range(0, len(unique_indivs_1)):\n",
    "        vecs = [unique_indivs_1[run], unique_indivs_2[run]]\n",
    "        min_lengths.append(min(len(min(vecs, key = len)), max_length) - 1)\n",
    "    return min_lengths\n",
    "\n",
    "def count_uniques_above_fitness_floor(unique_inds, min_lengths, fitness_floor):\n",
    "    count = []\n",
    "    for run, min_gen in zip(unique_inds, min_lengths):\n",
    "        run_count = 0\n",
    "        if min_gen < 0 :\n",
    "            continue\n",
    "        for indiv in run[min_gen]:\n",
    "            if indiv[\"fitness\"] > fitness_floor:\n",
    "                run_count += 1\n",
    "        count.append(run_count)\n",
    "    return count\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_floor =0.5\n",
    "\n",
    "fmni_new_vs_old = stats.ttest_ind(\n",
    "  count_uniques_above_fitness_floor(data_f1_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f1_t1, data_f2_t1), fitness_floor=fit_floor),\n",
    "  count_uniques_above_fitness_floor(data_f2_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f1_t1, data_f2_t1), fitness_floor=fit_floor),\n",
    "  equal_var=False\n",
    ")\n",
    "fmni_new_vs_old_no_crossover = stats.ttest_ind(\n",
    "  count_uniques_above_fitness_floor(data_f1_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f1_t1, data_f3_t1), fitness_floor=fit_floor),\n",
    "  count_uniques_above_fitness_floor(data_f3_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f1_t1, data_f3_t1), fitness_floor=fit_floor),\n",
    "  equal_var=False\n",
    ")\n",
    "fmni_new_cr_vs_old = stats.ttest_ind(\n",
    "  count_uniques_above_fitness_floor(data_f4_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f4_t1, data_f2_t1), fitness_floor=fit_floor),\n",
    "  count_uniques_above_fitness_floor(data_f2_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f4_t1, data_f2_t1), fitness_floor=fit_floor),\n",
    "  equal_var=False\n",
    ")\n",
    "fmni_new_cr_vs_old_no_crossover = stats.ttest_ind(\n",
    "  count_uniques_above_fitness_floor(data_f4_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f4_t1, data_f3_t1), fitness_floor=fit_floor),\n",
    "  count_uniques_above_fitness_floor(data_f3_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f4_t1, data_f3_t1), fitness_floor=fit_floor),\n",
    "  equal_var=False\n",
    ")\n",
    "fmni_new_cr_vs_new = stats.ttest_ind(\n",
    "  count_uniques_above_fitness_floor(data_f4_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f4_t1, data_f1_t1), fitness_floor=fit_floor),\n",
    "  count_uniques_above_fitness_floor(data_f1_t1[\"unique_indivs\"], find_last_unique_gen_in_common(data_f4_t1, data_f1_t1), fitness_floor=fit_floor),\n",
    "  equal_var=False\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmni_new_vs_old_no_crossover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmni_new_vs_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmni_new_cr_vs_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmni_new_cr_vs_old_no_crossover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmni_new_cr_vs_new\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond this point is code for archive analysis, this is not refined. Do not use unless necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_list = []\n",
    "for i in run_number:\n",
    "    it = 1\n",
    "    try:\n",
    "        while True:\n",
    "            archive = load_archive(path, i, it)\n",
    "            it += 1\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print(f\"loading archive {it - 1} for run {i}\")\n",
    "        archive = load_archive(path, i, it - 1)\n",
    "        for x in archive:\n",
    "            if \"fitness\" in archive[x]:\n",
    "                archive_list.append([x, archive[x], archive[x][\"fitness\"], i])\n",
    "    except:\n",
    "        print(f\"Run {i} has no archive\")\n",
    "archive_list.sort(key=lambda x: x[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_list\n",
    "with open(\"data.json\", \"w\") as f:\n",
    "    json.dump(archive_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "while it < 10:\n",
    "    print(archive_list[it])\n",
    "    it += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def turn_to_expr(phenotype, tree):\n",
    "    if phenotype == \"\":\n",
    "        return tree\n",
    "\n",
    "    if phenotype[0:9] == \"multiply(\":\n",
    "        # print(\"multiply\")\n",
    "        phenotype = phenotype[9:]\n",
    "        node = Node(\"*\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:14] == \"divide_no_nan(\":\n",
    "        phenotype = phenotype[14:]\n",
    "        node = Node(\"/\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:4] == \"add(\":\n",
    "        phenotype = phenotype[4:]\n",
    "        node = Node(\"+\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:4] == \"pow(\":\n",
    "        phenotype = phenotype[4:]\n",
    "        node = Node(\"^\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:9] == \"subtract(\":\n",
    "        phenotype = phenotype[9:]\n",
    "        node = Node(\"-\", tree, 2)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:4] == \"grad\":\n",
    "        phenotype = phenotype[4:]\n",
    "        node = Node(\"grad\", tree, 0)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:4] == \"beta\":\n",
    "        phenotype = phenotype[4:]\n",
    "        node = Node(\"beta\", tree, 0)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:5] == \"alpha\":\n",
    "        phenotype = phenotype[5:]\n",
    "        node = Node(\"alpha\", tree, 0)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:5] == \"sigma\":\n",
    "        phenotype = phenotype[5:]\n",
    "        node = Node(\"sigma\", tree, 0)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:9] == \"negative(\":\n",
    "        phenotype = phenotype[9:]\n",
    "        node = Node(\"-\", tree, 1)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:5] == \"sqrt(\":\n",
    "        phenotype = phenotype[5:]\n",
    "        node = Node(\"sqrt\", tree, 1)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:7] == \"square(\":\n",
    "        phenotype = phenotype[7:]\n",
    "        node = Node(\"square\", tree, 1)\n",
    "        tree.insert(node)\n",
    "        tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:2] == \", \":\n",
    "        phenotype = phenotype[2:]\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0] == \")\":\n",
    "        phenotype = phenotype[1:]\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    elif phenotype[0:9] == \"constant(\":\n",
    "        if (\n",
    "            phenotype[9:12] == \"0.0\"\n",
    "            or phenotype[9:13] == \"1.0)\"\n",
    "            or phenotype[9:13] == \"1.0,\"\n",
    "        ):\n",
    "            node = Node(phenotype[9:12], tree, 0)\n",
    "            phenotype = phenotype[12:]\n",
    "            tree.insert(node)\n",
    "            tree = tree.get_next()\n",
    "        else:\n",
    "            node = Node(phenotype[9 : 9 + 14], tree, 0)\n",
    "            phenotype = phenotype[9 + 14 :]\n",
    "            tree.insert(node)\n",
    "            tree = tree.get_next()\n",
    "        return turn_to_expr(phenotype, tree)\n",
    "    else:\n",
    "        raise Exception(phenotype)\n",
    "\n",
    "\n",
    "def math_phenotype(phenotype):\n",
    "    functions = trim_phenotype(phenotype)\n",
    "    alpha_func_string = functions[1][8:-2]\n",
    "    beta_func_string = functions[2][14:-2]\n",
    "    sigma_func_string = functions[3][21:-2]\n",
    "    grad_func_string = functions[-1][21:]\n",
    "\n",
    "    for x in [alpha_func_string, beta_func_string, sigma_func_string, grad_func_string]:\n",
    "        print(x)\n",
    "        turn_to_expr(x, Node(\"\", None, 1)).to_string()\n",
    "\n",
    "    return grad_func_string\n",
    "\n",
    "\n",
    "for indiv in data[\"indivs\"]:\n",
    "    math_phenotype(indiv[\"phenotype\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"multiply(\"\n",
    "print(a[0:9])\n",
    "print(a[9:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import csv\n",
    "from pickle import NONE\n",
    "from utils.data_functions import (\n",
    "    load_fashion_mnist_training,\n",
    "    load_cifar10_training,\n",
    "    load_mnist_training,\n",
    "    select_fashion_mnist_training,\n",
    ")\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "from keras.models import load_model\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from optimizers.custom_optimizer import CustomOptimizer\n",
    "import datetime\n",
    "\n",
    "experiment_time = datetime.datetime.now()\n",
    "\n",
    "cached_dataset = None\n",
    "cached_model = None\n",
    "\n",
    "\n",
    "def train_model_tensorflow_cifar10(phen_params):\n",
    "    phen, params = phen_params\n",
    "    validation_size = params[\"VALIDATION_SIZE\"]\n",
    "    fitness_size = params[\"FITNESS_SIZE\"]\n",
    "    batch_size = params[\"BATCH_SIZE\"]\n",
    "    epochs = params[\"EPOCHS\"]\n",
    "    patience = params[\"PATIENCE\"]\n",
    "\n",
    "    # Note that globals are borderline -- consider an object or a closure\n",
    "    # deliberately using globals() to make it ugly...\n",
    "    if globals()[\"cached_dataset\"] == None:\n",
    "        globals()[\"cached_dataset\"] = load_cifar10_training(\n",
    "            validation_size=validation_size, test_size=fitness_size\n",
    "        )\n",
    "\n",
    "    if globals()[\"cached_model\"] == None:\n",
    "        globals()[\"cached_model\"] = load_model(params[\"MODEL\"], compile=False)\n",
    "\n",
    "    # we assume validation and test sets are deterministic\n",
    "    dataset = globals()[\"cached_dataset\"]\n",
    "    model = tf.keras.models.clone_model(globals()[\"cached_model\"])\n",
    "\n",
    "    weights = model.get_weights()\n",
    "    model.set_weights(weights)\n",
    "\n",
    "    # optimizer is constant aslong as phen doesn't changed?\n",
    "    # -> opportunity to cache opt and compiled model\n",
    "    opt = CustomOptimizer(phen=phen, model=model)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\", patience=patience, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    score = model.fit(\n",
    "        dataset[\"x_train\"],\n",
    "        dataset[\"y_train\"],\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=2,\n",
    "        validation_data=(dataset[\"x_val\"], dataset[\"y_val\"]),\n",
    "        validation_steps=validation_size // batch_size,\n",
    "        callbacks=[early_stop],\n",
    "    )\n",
    "\n",
    "    K.clear_session()\n",
    "    results = {}\n",
    "    for metric in score.history:\n",
    "        results[metric] = []\n",
    "        for n in score.history[metric]:\n",
    "            results[metric].append(n)\n",
    "    test_score = model.evaluate(\n",
    "        x=dataset[\"x_test\"],\n",
    "        y=dataset[\"y_test\"],\n",
    "        verbose=0,\n",
    "        callbacks=[keras.callbacks.History()],\n",
    "    )\n",
    "    return test_score[-1], results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "0357653c69581ed709ad33b04b35df511a3a4051acb4aa15a7fef2257addd2ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
