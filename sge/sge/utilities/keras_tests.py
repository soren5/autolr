# -*- coding: utf-8 -*-
"""Keras tests.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bv3UZ70wPtLzo78xwCkNEJnOUA3aODGm
"""

import csv
import datetime
import time
import random
import numpy as np
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras import backend as K
from numpy.random import seed
import tensorflow as tf
from sklearn.model_selection import train_test_split
from bee_bot.flower import create_report

def prot_div(left, right):
    if right == 0:
        return 0
    else:
        return left / right

def if_func(condition, state1, state2):
    if condition:
        return state1
    else:
        return state2

batch_size = 1000
epochs = 100
validation_size = 7500

datagen_train = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True)

datagen_test = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True)



def resize_data(args):
    """
        Resize the dataset 28 x 28 datasets to 32x32

        Parameters
        ----------
        args : tuple(np.array, (int, int))
            instances, and shape of the reshaped signal

        Returns
        -------
        content : np.array
            reshaped instances
    """

    content, shape = args
    session = tf.compat.v1.Session()
    content = content.reshape(-1, 28, 28, 1)

    if shape != (28, 28):
        content = tf.image.resize(content, shape, tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    
    content = tf.image.grayscale_to_rgb(content)
    return content

def load_dataset(n_classes=10, validation_size=validation_size):
        #Confirmar mnist
        #(x_train, y_train), (x_test, y_test) = converted_data
        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,
                                                          test_size=validation_size,
                                                          stratify=y_train)


        #input scaling
        x_train = x_train.astype('float32')
        x_val = x_val.astype('float32')
        x_test = x_test.astype('float32')
        x_train /= 255
        x_val /= 255
        x_test /= 255

        #subraction of the mean image
        x_mean = 0
        for x in x_train:
            x_mean += x
        x_mean /= len(x_train)
        x_train -= x_mean
        x_val -= x_mean
        x_test -= x_mean


        x_train = resize_data((x_train, (32,32)))
        x_val = resize_data((x_val, (32,32)))
        x_test = resize_data((x_test, (32,32)))

        y_train = keras.utils.to_categorical(y_train, n_classes)
        y_val = keras.utils.to_categorical(y_val, n_classes)
        y_test = keras.utils.to_categorical(y_test, n_classes)

        dataset = { 'x_train': x_train,
                    'y_train': y_train,
                    'x_val': x_val,
                   'y_val': y_val,
                   'x_test': x_test,
                   'y_test': y_test}

        return dataset

dataset = load_dataset()
print(dataset['x_train'].shape)

datagen_train.fit(dataset['x_train'])
datagen_test.fit(dataset['x_train'])

def run_model(optimizer, scheduler=None):
    model = load_model('examples/models/model_7_0.h5')
    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    callbacks = []
    if scheduler != None:
        lr_schedule_callback = keras.callbacks.LearningRateScheduler(scheduler, verbose=1)
        callbacks.append(lr_schedule_callback)
    score = model.fit_generator(datagen_train.flow(dataset['x_train'],
                                                            dataset['y_train'],
                                                            batch_size=batch_size),
                                        steps_per_epoch=(dataset['x_train'].shape[0] // batch_size),
                                        epochs=epochs,
                                        validation_data=(datagen_test.flow(dataset['x_val'], dataset['y_val'], batch_size=batch_size)),
                                        validation_steps = validation_size // batch_size,
                                        callbacks = callbacks,
                                    verbose=1)
    test_score = model.evaluate(x=datagen_test.flow(dataset['x_test'], dataset['y_test'], batch_size=batch_size))
    return score, test_score

def fake_train_model(phen):
    training_epoch = int(random.uniform(0, 100))
    pain = {
        'val_loss': [random.uniform(0, 20) for i in range(training_epoch)],
        'val_acc': [random.uniform(0, 1) for i in range(training_epoch)]
    }
    test_score = random.uniform(0,1)
    return test_score, pain

results = {
    "sgd": [],
    "rmsprop": [],
    'adagrad': [],
    "adadelta": [],
    'adam': [],
    "adamax": [],
    "nadam": [],
}

optimizer_list = [
    ["sgd", keras.optimizers.SGD()],
    ["rmsprop", keras.optimizers.RMSprop()],
    ["adagrad", keras.optimizers.Adagrad()],
    ["adadelta", keras.optimizers.Adadelta()],
    ["adam", keras.optimizers.Adam()],
    ["adamax", keras.optimizers.Adamax()],
    ["nadam", keras.optimizers.Nadam()],
]

start_time = time.time()
counter = 0
total_calls = 5 * len(optimizer_list)


for i in range(5):
    for j in optimizer_list:
        optimizer_name = j[0]
        optimizer = j[1]
        training_results, test_results = run_model(optimizer)
        print(i, optimizer_name)
        #training_results, test_results = fake_train_model('0')
        counter += 1
        results[optimizer_name].append([training_results, test_results])
        progress_percentage = float(counter + 1) / total_calls
        elapsed_time = time.time() - start_time
        time_estimate = elapsed_time / progress_percentage
        print(time_estimate)

        hours_left = int(time_estimate / 60 / 60)
        minutes_left = int(time_estimate % 60)
        seconds_left = int(time_estimate / 60 % 60)
        create_report('optimizer_results.json', 'bee_reports/', 
            {'Percentage:': progress_percentage,
            'Time left:': str(hours_left) + ':' + str(minutes_left) + ':' + str(seconds_left)
            },
            results)
